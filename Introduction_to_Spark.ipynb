{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cD_RSYBltEGS"
   },
   "source": [
    "### Example 1 - Wordcount (Plain Map/Reduce)\n",
    "\n",
    "**Wordcount**: find the number of occurences of each word in a body of text.\n",
    "\n",
    "#### Data format\n",
    "The file `text.txt` contains the following short text:\n",
    "\n",
    "```\n",
    "this is a text file with random words like text , words , like this is an example of a text file\n",
    "```\n",
    "\n",
    "The data is stored in S3: `s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/text.txt`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2uaG6GoZthpT",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3423</td><td>application_1732639283265_3379</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3379/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-227.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3379_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text', 3), ('this', 2), ('is', 2), ('like', 2), ('a', 2), ('file', 2), ('words', 2), (',', 2), ('an', 1), ('of', 1), ('with', 1), ('random', 1), ('example', 1)]\n",
      "Time taken: 4.81 seconds"
     ]
    }
   ],
   "source": [
    "# Spark RDD code\n",
    "from pyspark.sql import SparkSession\n",
    "# To log our application's execution time:\n",
    "import time\n",
    "\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"wordcount example\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "# Lambda functions overload!\n",
    "wordcount = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/text.txt\") \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Collect and print the result\n",
    "print(wordcount.collect())\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8AUWoSxtEGZ"
   },
   "source": [
    "#### Code explanation\n",
    "- First, we create a `sparkSession` and a `SparkContex`:\n",
    "    - `sparkSession` is an entry point for every programming library in Spark and we need to create one in order to execute code.\n",
    "    - The `sparkContext` is an entry point specific for RDDs.\n",
    "\n",
    "- Then, the program reads the `text.txt` file from HDFS. With the use of a `lambda function` we split the data every time there is a whitespace between them.\n",
    "    - A lambda function is essentially an anonymous function we can use to write quick throwaway functions without defining or naming them.\n",
    "    - The lambda function the program uses as a `flatMap` argument: `Lambda x: x.split(\" \")`\n",
    "    - `flatMap` vs `map`: instead of creating multiple lists -> single list with all values.\n",
    "\n",
    "- Next, with the use of a `map` function we create a `(key,value)` pair for every word.\n",
    "    - We set `key = $word` and `value = 1`\n",
    "\n",
    "- We use the `reduceByKey` function: every tuple with the same `key` is sent to the same `reducer` so it **aggregate** them and create the result.\n",
    "    - In our case, if more than one tuples with the same `key` exist,  we **add** their `values`\n",
    "\n",
    "- Finally, we `sortBy` `value` (number of occurrence) and print the result.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlI0H8fKtEGb"
   },
   "source": [
    "### Example 2 - Simple Database\n",
    "#### Data format\n",
    "`employees.csv` contains the ID of the employee, the name of the employee, the salary of the employee and the ID of the department that the employee works at.\n",
    "\n",
    "| ID          | Name        | Salary      | DepartmentID |\n",
    "| ----------- | ----------- | ----------- | ------------ |\n",
    "| 1           | George R    | 2000        | 1            |\n",
    "\n",
    "`Departments.csv` contains the ID of the department and the name of the department.\n",
    "\n",
    "| ID          | Name        |\n",
    "| ----------- | ----------- |\n",
    "| 1           | Dep A       |\n",
    "\n",
    "The data is stored in S3:\n",
    "- `s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees.csv`\n",
    "- `s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/departments.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2SnsHl2tEGc",
    "tags": []
   },
   "source": [
    "#### QUERY 1: *Find the 5 worst paid employees*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PvMh9S_YveAp",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(550, ['6', 'Jerry L', '3']), (1000, ['2', 'John K', '2']), (1000, ['7', 'Marios K', '1']), (1050, ['5', 'Helen K', '2']), (1500, ['10', 'Yiannis T', '1'])]"
     ]
    }
   ],
   "source": [
    "# Spark RDD code\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 1 execution\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "start_time = time.time()\n",
    "# Experiment with map vs flatmap here:\n",
    "employees = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees.csv\") \\\n",
    "                .map(lambda x: (x.split(\",\")))\n",
    "sorted_employees = employees.map(lambda x: [int(x[2]), [x[0], x[1], x[3]] ]) \\\n",
    "                    .sortByKey()\n",
    "print(sorted_employees.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Omicu5ytEGh"
   },
   "source": [
    "Try to explain:\n",
    "-  `flatMmap` vs `map`: why `map` here?\n",
    "- what does the second `lambda` function do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Spark DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4JAwBSsRvo4c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- dep_id: integer (nullable = true)\n",
      "\n",
      "+---+---------+------+------+\n",
      "| id|     name|salary|dep_id|\n",
      "+---+---------+------+------+\n",
      "|  6|  Jerry L| 550.0|     3|\n",
      "|  2|   John K|1000.0|     2|\n",
      "|  7| Marios K|1000.0|     1|\n",
      "|  5|  Helen K|1050.0|     2|\n",
      "| 10|Yiannis T|1500.0|     1|\n",
      "+---+---------+------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 1 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"salary\", FloatType()),\n",
    "    StructField(\"dep_id\", IntegerType()),\n",
    "])\n",
    "\n",
    "employees_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees.csv\", header=False, schema=employees_schema)\n",
    "#print the schema of the DataFrame:\n",
    "employees_df.printSchema()\n",
    "\n",
    "## Alternative way to read csv:\n",
    "# employees_df = spark.read.format('csv') \\\n",
    "#     .options(header='false') \\\n",
    "#     .schema(employees_schema) \\\n",
    "#     .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees.csv\")\n",
    "\n",
    "sorted_employees_df = employees_df.sort(col(\"salary\"))\n",
    "sorted_employees_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWjNHPYDxeQe"
   },
   "source": [
    "Remember to use `explain()` to check if the physical plan is what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (4)\n",
      "+- Sort (3)\n",
      "   +- Exchange (2)\n",
      "      +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [4]: [id#0, name#1, salary#2, dep_id#3]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees.csv]\n",
      "ReadSchema: struct<id:int,name:string,salary:float,dep_id:int>\n",
      "\n",
      "(2) Exchange\n",
      "Input [4]: [id#0, name#1, salary#2, dep_id#3]\n",
      "Arguments: rangepartitioning(salary#2 ASC NULLS FIRST, 1000), ENSURE_REQUIREMENTS, [plan_id=13]\n",
      "\n",
      "(3) Sort\n",
      "Input [4]: [id#0, name#1, salary#2, dep_id#3]\n",
      "Arguments: [salary#2 ASC NULLS FIRST], true, 0\n",
      "\n",
      "(4) AdaptiveSparkPlan\n",
      "Output [4]: [id#0, name#1, salary#2, dep_id#3]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "## Use \"explain()\" to display physical plan:\n",
    "sorted_employees_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+------+\n",
      "| id|      name|salary|dep_id|\n",
      "+---+----------+------+------+\n",
      "|  6|   Jerry L| 550.0|     3|\n",
      "|  2|    John K|1000.0|     2|\n",
      "|  7|  Marios K|1000.0|     1|\n",
      "|  5|   Helen K|1050.0|     2|\n",
      "| 10| Yiannis T|1500.0|     1|\n",
      "|  1|  George R|2000.0|     1|\n",
      "|  3|    Mary T|2100.0|     1|\n",
      "|  4|  George T|2100.0|     1|\n",
      "|  8|  George K|2500.0|     2|\n",
      "| 11| Antonis T|2500.0|     2|\n",
      "|  9|Vasilios D|3500.0|     3|\n",
      "+---+----------+------+------+"
     ]
    }
   ],
   "source": [
    "# Write results to S3 -> \n",
    "#    1. create the output directory in your S3 bucket\n",
    "#    2. change your group number below \n",
    "#    3. and uncomment\n",
    "group_number = \"53\"\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/some_employees/\"\n",
    "sorted_employees_df.write.mode(\"overwrite\").parquet(s3_path)\n",
    "sorted_employees_df_again = spark.read.parquet(s3_path)\n",
    "sorted_employees_df_again.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw8Y_0qptEGj"
   },
   "source": [
    "___\n",
    "#### QUERY 2: *Find the 3 best paid employees from \"Dep A\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-meC7o8Ww0x4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2100, ['3', 'Mary T']), (2100, ['4', 'George T']), (2000, ['1', 'George R'])]"
     ]
    }
   ],
   "source": [
    "# Spark RDD code\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 2 execution\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "employees = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees.csv\") \\\n",
    "                .map(lambda x: (x.split(\",\")))\n",
    "departments = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/departments.csv\") \\\n",
    "                .map(lambda x: (x.split(\",\")))\n",
    "depA = departments.filter(lambda x: x[1] == \"Dep A\")\n",
    "# print(depA.collect())\n",
    "\n",
    "\n",
    "employees_formatted = employees.map(lambda x: [x[3] , [x[0],x[1],x[2]] ] )\n",
    "depA_formatted = depA.map(lambda x: [x[0], [x[1]]])\n",
    "# print(employees_formatted.collect())\n",
    "# print(depA_formatted.collect())\n",
    "\n",
    "joined_data = employees_formatted.join(depA_formatted)\n",
    "# print(joined_data.collect())\n",
    "\n",
    "get_employees = joined_data.map(lambda x: (x[1][0]))\n",
    "# print(get_employees.collect())\n",
    "\n",
    "sorted_employees= get_employees.map(lambda x: [int(x[2]),[x[0], x[1]] ] ) \\\n",
    "                                .sortByKey(ascending=False)\n",
    "print(sorted_employees.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WW5_0kkttEGn",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    name|salary|\n",
      "+--------+------+\n",
      "|  Mary T|2100.0|\n",
      "|George T|2100.0|\n",
      "|George R|2000.0|\n",
      "+--------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "/mnt/yarn/usercache/livy/appcache/application_1732639283265_0195/container_1732639283265_0195_01_000001/pyspark.zip/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead."
     ]
    }
   ],
   "source": [
    "# Spark SQL code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 2 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"salary\", FloatType()),\n",
    "    StructField(\"dep_id\", IntegerType()),\n",
    "])\n",
    "\n",
    "employees_df = spark.read.format('csv') \\\n",
    "    .options(header='false') \\\n",
    "    .schema(employees_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees.csv\")\n",
    "\n",
    "departments_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "])\n",
    "\n",
    "departments_df = spark.read.format('csv') \\\n",
    "    .options(header='false') \\\n",
    "    .schema(departments_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/departments.csv\")\n",
    "\n",
    "# To utilize as SQL tables\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "id_query = \"SELECT departments.id, departments.name \\\n",
    "    FROM departments \\\n",
    "    WHERE departments.name == 'Dep A'\"\n",
    "\n",
    "depA_id = spark.sql(id_query)\n",
    "# This works but is deprecated\n",
    "depA_id.registerTempTable(\"depA\")\n",
    "inner_join_query = \"SELECT employees.name, employees.salary \\\n",
    "    FROM employees \\\n",
    "    INNER JOIN depA ON employees.dep_id == depA.id \\\n",
    "    ORDER BY employees.salary DESC\"\n",
    "\n",
    "joined_data = spark.sql(inner_join_query)\n",
    "joined_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- Sort (10)\n",
      "   +- Exchange (9)\n",
      "      +- Project (8)\n",
      "         +- BroadcastHashJoin Inner BuildRight (7)\n",
      "            :- Filter (2)\n",
      "            :  +- Scan csv  (1)\n",
      "            +- BroadcastExchange (6)\n",
      "               +- Project (5)\n",
      "                  +- Filter (4)\n",
      "                     +- Scan csv  (3)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [3]: [name#42, salary#43, dep_id#44]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees.csv]\n",
      "PushedFilters: [IsNotNull(dep_id)]\n",
      "ReadSchema: struct<name:string,salary:float,dep_id:int>\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [name#42, salary#43, dep_id#44]\n",
      "Condition : isnotnull(dep_id#44)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [2]: [id#49, name#50]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/departments.csv]\n",
      "PushedFilters: [IsNotNull(name), EqualTo(name,Dep A), IsNotNull(id)]\n",
      "ReadSchema: struct<id:int,name:string>\n",
      "\n",
      "(4) Filter\n",
      "Input [2]: [id#49, name#50]\n",
      "Condition : ((isnotnull(name#50) AND (name#50 = Dep A)) AND isnotnull(id#49))\n",
      "\n",
      "(5) Project\n",
      "Output [1]: [id#49]\n",
      "Input [2]: [id#49, name#50]\n",
      "\n",
      "(6) BroadcastExchange\n",
      "Input [1]: [id#49]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=142]\n",
      "\n",
      "(7) BroadcastHashJoin\n",
      "Left keys [1]: [dep_id#44]\n",
      "Right keys [1]: [id#49]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(8) Project\n",
      "Output [2]: [name#42, salary#43]\n",
      "Input [4]: [name#42, salary#43, dep_id#44, id#49]\n",
      "\n",
      "(9) Exchange\n",
      "Input [2]: [name#42, salary#43]\n",
      "Arguments: rangepartitioning(salary#43 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=146]\n",
      "\n",
      "(10) Sort\n",
      "Input [2]: [name#42, salary#43]\n",
      "Arguments: [salary#43 DESC NULLS LAST], true, 0\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [2]: [name#42, salary#43]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "joined_data.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to change the join strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    name|salary|\n",
      "+--------+------+\n",
      "|  Mary T|2100.0|\n",
      "|George T|2100.0|\n",
      "|George R|2000.0|\n",
      "+--------+------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "inner_join_query = \"SELECT /*+ SHUFFLE_HASH(depA) */ employees.name, employees.salary \\\n",
    "    FROM employees \\\n",
    "    INNER JOIN depA ON employees.dep_id = depA.id \\\n",
    "    ORDER BY employees.salary DESC\"\n",
    "\n",
    "joined_data = spark.sql(inner_join_query)\n",
    "joined_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (12)\n",
      "+- Sort (11)\n",
      "   +- Exchange (10)\n",
      "      +- Project (9)\n",
      "         +- ShuffledHashJoin Inner BuildRight (8)\n",
      "            :- Exchange (3)\n",
      "            :  +- Filter (2)\n",
      "            :     +- Scan csv  (1)\n",
      "            +- Exchange (7)\n",
      "               +- Project (6)\n",
      "                  +- Filter (5)\n",
      "                     +- Scan csv  (4)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [3]: [name#76, salary#77, dep_id#78]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees.csv]\n",
      "PushedFilters: [IsNotNull(dep_id)]\n",
      "ReadSchema: struct<name:string,salary:float,dep_id:int>\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [name#76, salary#77, dep_id#78]\n",
      "Condition : isnotnull(dep_id#78)\n",
      "\n",
      "(3) Exchange\n",
      "Input [3]: [name#76, salary#77, dep_id#78]\n",
      "Arguments: hashpartitioning(dep_id#78, 1000), ENSURE_REQUIREMENTS, [plan_id=338]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [2]: [id#83, name#84]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/departments.csv]\n",
      "PushedFilters: [IsNotNull(name), EqualTo(name,Dep A), IsNotNull(id)]\n",
      "ReadSchema: struct<id:int,name:string>\n",
      "\n",
      "(5) Filter\n",
      "Input [2]: [id#83, name#84]\n",
      "Condition : ((isnotnull(name#84) AND (name#84 = Dep A)) AND isnotnull(id#83))\n",
      "\n",
      "(6) Project\n",
      "Output [1]: [id#83]\n",
      "Input [2]: [id#83, name#84]\n",
      "\n",
      "(7) Exchange\n",
      "Input [1]: [id#83]\n",
      "Arguments: hashpartitioning(id#83, 1000), ENSURE_REQUIREMENTS, [plan_id=339]\n",
      "\n",
      "(8) ShuffledHashJoin\n",
      "Left keys [1]: [dep_id#78]\n",
      "Right keys [1]: [id#83]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(9) Project\n",
      "Output [2]: [name#76, salary#77]\n",
      "Input [4]: [name#76, salary#77, dep_id#78, id#83]\n",
      "\n",
      "(10) Exchange\n",
      "Input [2]: [name#76, salary#77]\n",
      "Arguments: rangepartitioning(salary#77 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=343]\n",
      "\n",
      "(11) Sort\n",
      "Input [2]: [name#76, salary#77]\n",
      "Arguments: [salary#77 DESC NULLS LAST], true, 0\n",
      "\n",
      "(12) AdaptiveSparkPlan\n",
      "Output [2]: [name#76, salary#77]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "joined_data.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfEt9QFEtEGo"
   },
   "source": [
    "___\n",
    "### Example 3 - Simple Database with a twist (DataFrame UDFs)\n",
    "\n",
    "Sometimes we need to define functions that process the values of specific columns of a single row.\n",
    "\n",
    "Motivating example: a database with salaries and bonuses for our employees:\n",
    "\n",
    "| ID          | Name        | Salary      | DepartmentID | Bonus        |\n",
    "| ----------- | ----------- | ----------- | ------------ | ------------ |\n",
    "| 1           | George R    | 2000        | 1            | 500          |\n",
    "\n",
    "The data is stored in S3:\n",
    "- `s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees2.csv`\n",
    "\n",
    "#### QUERY: *Calculate the total yearly income for each employee: `14 x Salary + Bonus`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4fCm45pjzBug",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['George R', 28500], ['John K', 14150], ['Mary T', 29850], ['George T', 29720], ['Helen K', 14900], ['Jerry L', 7900], ['Marios K', 14550], ['George K', 36500], ['Vasilios D', 50000], ['Yiannis T', 21450], ['Antonis T', 35620]]"
     ]
    }
   ],
   "source": [
    "# Spark RDD code: the 'map' function is enough\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 1 execution\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "employees = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees2.csv\") \\\n",
    "                .map(lambda x: (x.split(\",\")))\n",
    "employees_yearly = employees.map(lambda x: [x[1], 14*(int(x[2]))+int(x[4])])\n",
    "print(employees_yearly.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "L4BziM-Oz1V3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      name| yearly|\n",
      "+----------+-------+\n",
      "|  George R|28500.0|\n",
      "|    John K|14150.0|\n",
      "|    Mary T|29850.0|\n",
      "|  George T|29720.0|\n",
      "|   Helen K|14900.0|\n",
      "|   Jerry L| 7900.0|\n",
      "|  Marios K|14550.0|\n",
      "|  George K|36500.0|\n",
      "|Vasilios D|50000.0|\n",
      "| Yiannis T|21450.0|\n",
      "| Antonis T|35620.0|\n",
      "+----------+-------+"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame code - UDF\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UDF example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "employees2_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"salary\", FloatType()),\n",
    "    StructField(\"dep_id\", IntegerType()),\n",
    "    StructField(\"bonus\", FloatType()),\n",
    "])\n",
    "\n",
    "def calculate_yearly_income(salary, bonus):\n",
    "    return 14*salary+bonus\n",
    "\n",
    "# Register the UDF\n",
    "calculate_yearly_income_udf = udf(calculate_yearly_income, FloatType())\n",
    "\n",
    "employees_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees2.csv\", header=False, schema=employees2_schema)\n",
    "\n",
    "employees_yearly_df = employees_df \\\n",
    "    .withColumn(\"yearly\", calculate_yearly_income_udf(col(\"salary\"), col(\"bonus\"))).select(\"name\", \"yearly\")\n",
    "\n",
    "employees_yearly_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### One Final Thing: Configuring our Spark Application Resources in Jupyter with SparkMagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: None\n",
      "Executor Memory: 4743M\n",
      "Executor Cores: 2"
     ]
    }
   ],
   "source": [
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>227</td><td>application_1732639283265_0196</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_0196/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_0196_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '1', 'spark.executor.memory': '1g', 'spark.executor.cores': '1', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>227</td><td>application_1732639283265_0196</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_0196/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_0196_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"1\",\n",
    "        \"spark.executor.memory\": \"1g\",\n",
    "        \"spark.executor.cores\": \"1\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 1\n",
      "Executor Memory: 1g\n",
      "Executor Cores: 1"
     ]
    }
   ],
   "source": [
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "name": "SparkLab - Introduction to RDDs and DataFrames"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
