{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e72566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Adults', 121093), ('Young adults', 38703), ('Children', 10830), ('Elders', 5985)]\n",
      "Time taken: 24.65 seconds"
     ]
    }
   ],
   "source": [
    "# Spark RDD code\n",
    "from pyspark.sql import SparkSession\n",
    "# To log our application's execution time:\n",
    "import time\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "#Spark Session Config for 4 executors\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 1 execution\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "def parse_csv_line(line):\n",
    "    f = StringIO(line)\n",
    "    reader = csv.reader(f)\n",
    "    return next(reader)\n",
    "\n",
    "# Age group converter\n",
    "def age_group(data):\n",
    "    try:\n",
    "        age=int(data)\n",
    "        if age < 18 and age > 0:\n",
    "            return \"Children\"\n",
    "        if age<25:\n",
    "            return \"Young adults\"\n",
    "        if age<65 :\n",
    "            return \"Adults\"\n",
    "        if age>64:\n",
    "            return \"Elders\"\n",
    "        else:\n",
    "            return \"No individual victim\"\n",
    "    except:\n",
    "        return \"error\"\n",
    "    \n",
    "#Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "#Read data from both datasets (10-19 and 20-present)\n",
    "data_rdd = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_*.csv\") \\\n",
    "             .map(parse_csv_line)\n",
    "#Remove Header\n",
    "header = data_rdd.first()\n",
    "data_rdd = data_rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Search for Aggravated, Convert by Age group and Sum in Descending order\n",
    "crime_data = data_rdd \\\n",
    "    .filter(lambda row: \"AGGRAVATED\" in row[9]) \\\n",
    "    .map(lambda row: (age_group(row[11]), 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda pair: pair[1], ascending=False)\n",
    "\n",
    "print(crime_data.collect())\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5b4a7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: None\n",
      "+------------+------+\n",
      "|   Age_group| count|\n",
      "+------------+------+\n",
      "|      Adults|121093|\n",
      "|Young adults| 38703|\n",
      "|    Children| 10830|\n",
      "|      Elders|  5985|\n",
      "+------------+------+\n",
      "\n",
      "Time taken: 12.97 seconds"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType, TimestampType\n",
    "from pyspark.sql.functions import col, udf\n",
    "import time\n",
    "import csv\n",
    "from io import StringIO\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "#Spark Session Config for 4 executors\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 1 execution\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "conf = spark.sparkContext.getConf()\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "\n",
    "\n",
    "# Age group converter\n",
    "def age_group(data):\n",
    "    try:\n",
    "        age=int(data)\n",
    "        if age < 18 and age > 0:\n",
    "            return \"Children\"\n",
    "        if age<25:\n",
    "            return \"Young adults\"\n",
    "        if age<65 :\n",
    "            return \"Adults\"\n",
    "        if age>64:\n",
    "            return \"Elders\"\n",
    "        else:\n",
    "            return \"No individual victim\"\n",
    "    except:\n",
    "        return \"error\"\n",
    "\n",
    "#Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Load Crime Data into DataFrame\n",
    "crime_df = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_*.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "#Filter AGGRAVATEED ASSAULT\n",
    "aggravated_df = crime_df.filter(col(\"Crm Cd Desc\").like(\"%AGGRAVATED ASSAULT%\"))\n",
    "\n",
    "# UDF AgeGroup\n",
    "age_group_udf = udf(age_group, StringType())\n",
    "aggravated_df = aggravated_df.withColumn(\"Age_group\", age_group_udf(col(\"Vict Age\")))\n",
    "age_group_count_df = aggravated_df.groupBy(\"Age_group\").count()\n",
    "result_df = age_group_count_df.orderBy(desc(\"count\"))\n",
    "\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a21e5-22f9-43b3-854e-ba9090609548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
