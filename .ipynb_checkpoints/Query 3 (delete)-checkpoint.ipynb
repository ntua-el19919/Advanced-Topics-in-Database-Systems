{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a62f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dbf2558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 3643 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "25/01/19 14:30:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/19 14:30:48 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-192-168-1-36.eu-central-1.compute.internal/192.168.1.36:8032\n",
      "25/01/19 14:30:49 INFO Configuration: resource-types.xml not found\n",
      "25/01/19 14:30:49 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/01/19 14:30:49 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "25/01/19 14:30:49 INFO Client: Will allocate AM container, with 1384 MB memory including 384 MB overhead\n",
      "25/01/19 14:30:49 INFO Client: Setting up container launch context for our AM\n",
      "25/01/19 14:30:49 INFO Client: Setting up the launch environment for our AM container\n",
      "25/01/19 14:30:49 INFO Client: Preparing resources for our AM container\n",
      "25/01/19 14:30:49 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/19 14:30:50 INFO Client: Uploading resource file:/mnt/tmp/spark-99804f56-247d-40aa-bbc2-b812e212088b/__spark_libs__9410590250472364106.zip -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/__spark_libs__9410590250472364106.zip\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/jars/sedona-spark-shaded-3.5_2.12-1.6.1.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/sedona-spark-shaded-3.5_2.12-1.6.1.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/jars/geotools-wrapper-1.6.1-28.2.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/geotools-wrapper-1.6.1-28.2.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/kryo-shaded-4.0.2.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.8.0-incubating.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/livy-api-0.8.0-incubating.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.8.0-incubating.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/livy-rsc-0.8.0-incubating.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.8.0-incubating.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/livy-thriftserver-session-0.8.0-incubating.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/minlog-1.3.0.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-all-4.1.100.Final.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-buffer-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-buffer-4.1.100.Final.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-4.1.100.Final.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-dns-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-dns-4.1.100.Final.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-haproxy-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-haproxy-4.1.100.Final.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-http-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-http-4.1.100.Final.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-http2-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-http2-4.1.100.Final.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-memcache-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-memcache-4.1.100.Final.jar\n",
      "25/01/19 14:30:51 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-mqtt-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-mqtt-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-redis-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-redis-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-smtp-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-smtp-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-socks-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-socks-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-stomp-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-stomp-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-xml-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-codec-xml-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-common-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-common-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-handler-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-handler-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-handler-proxy-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-handler-proxy-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-handler-ssl-ocsp-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-handler-ssl-ocsp-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-resolver-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-dns-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-resolver-dns-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-dns-classes-macos-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-resolver-dns-classes-macos-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-transport-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-classes-epoll-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-transport-classes-epoll-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-classes-kqueue-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-transport-classes-kqueue-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-native-unix-common-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-transport-native-unix-common-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-rxtx-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-transport-rxtx-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-sctp-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-transport-sctp-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-udt-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/netty-transport-udt-4.1.100.Final.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/objenesis-2.5.1.jar\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/commons-codec-1.9.jar\n",
      "25/01/19 14:30:52 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "25/01/19 14:30:52 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.8.0-incubating.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/livy-core_2.12-0.8.0-incubating.jar\n",
      "25/01/19 14:30:53 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.8.0-incubating.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/livy-repl_2.12-0.8.0-incubating.jar\n",
      "25/01/19 14:30:53 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "25/01/19 14:30:53 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "25/01/19 14:30:53 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/hive-site.xml\n",
      "25/01/19 14:30:53 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/hudi-defaults.conf\n",
      "25/01/19 14:30:53 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/sparkr.zip\n",
      "25/01/19 14:30:53 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/pyspark.zip\n",
      "25/01/19 14:30:53 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/py4j-0.10.9.7-src.zip\n",
      "25/01/19 14:30:53 INFO Client: Uploading resource file:/mnt/tmp/spark-99804f56-247d-40aa-bbc2-b812e212088b/__spark_conf__12819562796212123840.zip -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_3599/__spark_conf__.zip\n",
      "25/01/19 14:30:53 INFO SecurityManager: Changing view acls to: livy\n",
      "25/01/19 14:30:53 INFO SecurityManager: Changing modify acls to: livy\n",
      "25/01/19 14:30:53 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/19 14:30:53 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/19 14:30:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "25/01/19 14:30:53 INFO Client: Submitting application application_1732639283265_3599 to ResourceManager\n",
      "25/01/19 14:30:53 INFO YarnClientImpl: Submitted application application_1732639283265_3599\n",
      "25/01/19 14:30:53 INFO Client: Application report for application_1732639283265_3599 (state: ACCEPTED)\n",
      "25/01/19 14:30:53 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Sun Jan 19 14:30:53 +0000 2025] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:196608, vCores:64> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 99.34895 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:196608, vCores:64> ; Queue's used capacity (absolute resource) = <memory:195328, vCores:42> ; Queue's max capacity (absolute resource) = <memory:196608, vCores:64> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: root.default\n",
      "\t start time: 1737297053564\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3599/\n",
      "\t user: livy\n",
      "25/01/19 14:30:53 INFO ShutdownHookManager: Shutdown hook called\n",
      "25/01/19 14:30:53 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-3ec01324-71c8-4981-bb3b-0dc5e8fde47b\n",
      "25/01/19 14:30:53 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-99804f56-247d-40aa-bbc2-b812e212088b\n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1732639283265_3599 failed 1 times (global limit =2; local limit is =1) due to AM Container for appattempt_1732639283265_3599_000001 exited with  exitCode: 137\n",
      "Failing this attempt.Diagnostics: [2025-01-19 14:55:56.887]Container killed on request. Exit code is 137\n",
      "[2025-01-19 14:55:56.887]Container exited with a non-zero exit code 137. \n",
      "[2025-01-19 14:55:56.887]Killed by external signal\n",
      "For more detailed output, check the application tracking page: http://ip-192-168-1-36.eu-central-1.compute.internal:8088/cluster/app/application_1732639283265_3599 Then click on links to logs of each attempt.\n",
      ". Failing the application.\n"
     ]
    }
   ],
   "source": [
    "#Query 3 Sedona for LA\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, broadcast, date_format, to_timestamp, round, format_number, concat, lit, when, count, sum, regexp_replace, trim\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "#Query 3 Initialisation with Dataframe\n",
    "start_time = time.time()\n",
    "\n",
    "# Crime dataframe with parquet for quicker esults\n",
    "crime_df = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group12/main_dataset_parquet\")\\\n",
    "                .filter((col(\"LON\") != 0) | (col(\"LAT\") != 0))  \\\n",
    "                .withColumn(\"point\",ST_Point(\"LON\", \"LAT\")) \\\n",
    "                .select(\"point\")\n",
    "\n",
    "# Income dataframe with median income\n",
    "income_df = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "res_income = income_df.withColumn(\"Estimated_Median_Income\",\n",
    "    regexp_replace(trim(col(\"Estimated Median Income\")), \"[^0-9]\", \"\").cast(IntegerType())\n",
    "    ).drop(\"Community\",\"Estimated Median Income\")\n",
    "\n",
    "# Filter population for total population anf houses\n",
    "zip_comm_population = flattened_df.select(\"ZCTA10\", \"COMM\", \"POP_2010\", \"HOUSING10\", \"geometry\") \\\n",
    "    .filter((col(\"ZCTA10\") > 0) & (col(\"POP_2010\") > 0) & (col(\"HOUSING10\") > 0) & (trim(col(\"COMM\")) != \"\")) \\\n",
    "    .groupBy(\"ZCTA10\", \"COMM\") \\\n",
    "    .agg(\n",
    "        sum(\"POP_2010\").alias(\"Total_Population_Zip_COMM\"),\n",
    "        sum(\"HOUSING10\").alias(\"Total_Housing\"),\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "    )\n",
    "    \n",
    "# Join incoma and cansus using zip coded\n",
    "\n",
    "joined_income_df = zip_comm_population.join(\n",
    "   res_income,      #BROADCAST\n",
    "    zip_comm_population[\"ZCTA10\"] == res_income[\"Zip Code\"],\n",
    "    \"inner\"  \n",
    "    ) \\\n",
    "    .drop(\"ZCTA10\",\"Zip Code\") \\\n",
    "    .withColumn(\"Total income\",col(\"Total_Housing\") * col(\"Estimated_Median_Income\")) \\\n",
    "    .drop(\"Estimated_Median_Income\") \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(sum(\"Total_Population_Zip_COMM\")\n",
    "    .alias(\"Total_Population\"),sum(\"Total income\")\n",
    "    .alias(\"Total Income\"),ST_Union_Aggr(\"geometry\")\n",
    "    .alias(\"geometry\")) \n",
    "\n",
    "\n",
    "results = joined_income_df.join(\n",
    "                crime_df,    #BROADCAST\n",
    "                ST_Within(crime_df.point, \n",
    "                joined_income_df.geometry), \"inner\") \\\n",
    "                .groupBy(\"COMM\",\"Total_Population\",\"Total Income\") \\\n",
    "                .agg(count(col(\"*\")).alias(\"Total crimes\")) \\\n",
    "                .withColumn(\"crime_rate\",format_number((col(\"Total crimes\")/col(\"Total_Population\")),4)) \\\n",
    "                .withColumn(\"estimated_median_income\",concat(lit(\"$\"),format_number(round(col(\"Total Income\") / col(\"Total_Population\")), 0))) \\\n",
    "                .drop(\"Total Income\",\"Total_Population\",\"Total crimes\") \\\n",
    "                .withColumnRenamed(\"COMM\", \"Location\") \\\n",
    "                .orderBy(col(\"crime_rate\").desc())\n",
    "results.show() \n",
    "end_time = time.time()\n",
    "# End timing\n",
    "print(f\"Execution Time using BROADCAST : {end_time - start_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2742b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint BROADCAST and Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint MERGE and Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99518b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint SHUFFLE_HASH and Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece90983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint Broadcast and Explain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
