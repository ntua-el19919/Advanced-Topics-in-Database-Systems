{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69d42682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-----------------------+\n",
      "|          Location|crime_rate|estimated_median_income|\n",
      "+------------------+----------+-----------------------+\n",
      "|      Little Tokyo|    2.9024|                $16,505|\n",
      "|          Downtown|    1.9612|                $19,908|\n",
      "|Wholesale District|    1.5441|                 $9,848|\n",
      "|         Hollywood|    1.3846|                $25,729|\n",
      "|     Vermont Vista|    1.2551|                 $8,367|\n",
      "|Century Palms/Cove|    1.1446|                 $8,610|\n",
      "|   Sycamore Square|    1.1276|                $30,117|\n",
      "| Manchester Square|    1.0804|                $14,590|\n",
      "|    Vermont Knolls|    1.0672|                $10,494|\n",
      "|    Gramercy Place|    1.0648|                $14,937|\n",
      "|Florence-Firestone|    1.0589|                 $8,079|\n",
      "|      Leimert Park|    1.0561|                $16,104|\n",
      "|            Venice|    1.0404|                $47,625|\n",
      "|       West Vernon|    1.0377|                 $8,723|\n",
      "|         Hyde Park|    1.0300|                $14,144|\n",
      "|     Green Meadows|    1.0210|                 $8,030|\n",
      "|       Rancho Park|    1.0086|                $39,038|\n",
      "|      Harvard Park|    1.0058|                 $9,844|\n",
      "|     Baldwin Hills|    0.9950|                $17,304|\n",
      "| Wellington Square|    0.9696|                $13,452|\n",
      "+------------------+----------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time : 53.924115896224976 seconds"
     ]
    }
   ],
   "source": [
    "#Query 3 Sedona for LA\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType, TimestampType\n",
    "from pyspark.sql.functions import col, broadcast, date_format, to_timestamp, round, format_number, concat, lit, when, count, sum, regexp_replace, trim\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Initial Code\") \\\n",
    "    .getOrCreate()\n",
    "spark.catalog.clearCache()\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "#Query 3 Initialisation with Dataframe\n",
    "start_time = time.time()\n",
    "\n",
    "# Crime dataframe with parquet for quicker esults\n",
    "crime_df = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group12/main_dataset_parquet\")\\\n",
    "                .filter((col(\"LON\") != 0) | (col(\"LAT\") != 0))  \\\n",
    "                .withColumn(\"point\",ST_Point(\"LON\", \"LAT\")) \\\n",
    "                .select(\"point\")\n",
    "\n",
    "# Income dataframe with median income\n",
    "income_df = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "res_income = income_df.withColumn(\"Estimated_Median_Income\",\n",
    "    regexp_replace(trim(col(\"Estimated Median Income\")), \"[^0-9]\", \"\").cast(IntegerType())\n",
    "    ).drop(\"Community\",\"Estimated Median Income\")\n",
    "\n",
    "# Filter population for total population anf houses\n",
    "zip_comm_population = flattened_df.select(\"ZCTA10\", \"COMM\", \"POP_2010\", \"HOUSING10\", \"geometry\") \\\n",
    "    .filter((col(\"ZCTA10\") > 0) & (col(\"POP_2010\") > 0) & (col(\"HOUSING10\") > 0) & (trim(col(\"COMM\")) != \"\")) \\\n",
    "    .groupBy(\"ZCTA10\", \"COMM\") \\\n",
    "    .agg(\n",
    "        sum(\"POP_2010\").alias(\"Total_Population_Zip_COMM\"),\n",
    "        sum(\"HOUSING10\").alias(\"Total_Housing\"),\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "    )\n",
    "    \n",
    "# Join incoma and cansus using zip coded\n",
    "\n",
    "joined_income_df = zip_comm_population.join(\n",
    "   res_income,      #BROADCAST\n",
    "    zip_comm_population[\"ZCTA10\"] == res_income[\"Zip Code\"],\n",
    "    \"inner\"  \n",
    "    ) \\\n",
    "    .drop(\"ZCTA10\",\"Zip Code\") \\\n",
    "    .withColumn(\"Total income\",col(\"Total_Housing\") * col(\"Estimated_Median_Income\")) \\\n",
    "    .drop(\"Estimated_Median_Income\") \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(sum(\"Total_Population_Zip_COMM\")\n",
    "    .alias(\"Total_Population\"),sum(\"Total income\")\n",
    "    .alias(\"Total Income\"),ST_Union_Aggr(\"geometry\")\n",
    "    .alias(\"geometry\")) \n",
    "\n",
    "\n",
    "results = joined_income_df.join(\n",
    "                crime_df,    #BROADCAST\n",
    "                ST_Within(crime_df.point, \n",
    "                joined_income_df.geometry), \"inner\") \\\n",
    "                .groupBy(\"COMM\",\"Total_Population\",\"Total Income\") \\\n",
    "                .agg(count(col(\"*\")).alias(\"Total crimes\")) \\\n",
    "                .withColumn(\"crime_rate\",format_number((col(\"Total crimes\")/col(\"Total_Population\")),4)) \\\n",
    "                .withColumn(\"estimated_median_income\",concat(lit(\"$\"),format_number(round(col(\"Total Income\") / col(\"Total_Population\")), 0))) \\\n",
    "                .drop(\"Total Income\",\"Total_Population\",\"Total crimes\") \\\n",
    "                .withColumnRenamed(\"COMM\", \"Location\") \\\n",
    "                .orderBy(col(\"crime_rate\").desc())\n",
    "results.show() \n",
    "end_time = time.time()\n",
    "# End timing\n",
    "print(f\"Execution Time : {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec4d27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (29)\n",
      "+- Sort (28)\n",
      "   +- Exchange (27)\n",
      "      +- HashAggregate (26)\n",
      "         +- Exchange (25)\n",
      "            +- HashAggregate (24)\n",
      "               +- Project (23)\n",
      "                  +- RangeJoin (22)\n",
      "                     :- Filter (18)\n",
      "                     :  +- ObjectHashAggregate (17)\n",
      "                     :     +- Exchange (16)\n",
      "                     :        +- ObjectHashAggregate (15)\n",
      "                     :           +- Project (14)\n",
      "                     :              +- BroadcastHashJoin Inner BuildRight (13)\n",
      "                     :                 :- ObjectHashAggregate (8)\n",
      "                     :                 :  +- Exchange (7)\n",
      "                     :                 :     +- ObjectHashAggregate (6)\n",
      "                     :                 :        +- Project (5)\n",
      "                     :                 :           +- Filter (4)\n",
      "                     :                 :              +- Generate (3)\n",
      "                     :                 :                 +- Filter (2)\n",
      "                     :                 :                    +- Scan geojson  (1)\n",
      "                     :                 +- BroadcastExchange (12)\n",
      "                     :                    +- Project (11)\n",
      "                     :                       +- Filter (10)\n",
      "                     :                          +- Scan csv  (9)\n",
      "                     +- Project (21)\n",
      "                        +- Filter (20)\n",
      "                           +- Scan parquet  (19)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#2257]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#2257]\n",
      "Condition : ((size(features#2257, true) > 0) AND isnotnull(features#2257))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#2257]\n",
      "Arguments: explode(features#2257), false, [features#2265]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#2265]\n",
      "Condition : ((((isnotnull(features#2265.properties.CITY) AND isnotnull(features#2265.properties.ZCTA10)) AND isnotnull(features#2265.properties.POP_2010)) AND isnotnull(features#2265.properties.HOUSING10)) AND ((features#2265.properties.CITY = Los Angeles) AND ((((cast(features#2265.properties.ZCTA10 as int) > 0) AND (features#2265.properties.POP_2010 > 0)) AND (features#2265.properties.HOUSING10 > 0)) AND NOT (trim(features#2265.properties.COMM, None) = ))))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#2265.properties.ZCTA10 AS ZCTA10#2298, features#2265.properties.COMM AS COMM#2281, features#2265.properties.POP_2010 AS POP_2010#2290L, features#2265.properties.HOUSING10 AS HOUSING10#2287L, features#2265.geometry AS geometry#2268]\n",
      "Input [1]: [features#2265]\n",
      "\n",
      "(6) ObjectHashAggregate\n",
      "Input [5]: [ZCTA10#2298, COMM#2281, POP_2010#2290L, HOUSING10#2287L, geometry#2268]\n",
      "Keys [2]: [ZCTA10#2298, COMM#2281]\n",
      "Functions [3]: [partial_sum(POP_2010#2290L), partial_sum(HOUSING10#2287L), partial_st_union_aggr(geometry#2268, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@c0a321f, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#2677L, sum#2679L, buf#2681]\n",
      "Results [5]: [ZCTA10#2298, COMM#2281, sum#2678L, sum#2680L, buf#2682]\n",
      "\n",
      "(7) Exchange\n",
      "Input [5]: [ZCTA10#2298, COMM#2281, sum#2678L, sum#2680L, buf#2682]\n",
      "Arguments: hashpartitioning(ZCTA10#2298, COMM#2281, 1000), ENSURE_REQUIREMENTS, [plan_id=1693]\n",
      "\n",
      "(8) ObjectHashAggregate\n",
      "Input [5]: [ZCTA10#2298, COMM#2281, sum#2678L, sum#2680L, buf#2682]\n",
      "Keys [2]: [ZCTA10#2298, COMM#2281]\n",
      "Functions [3]: [sum(POP_2010#2290L), sum(HOUSING10#2287L), st_union_aggr(geometry#2268, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@c0a321f, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#2290L)#2532L, sum(HOUSING10#2287L)#2534L, ST_Union_Aggr(geometry#2268)#2539]\n",
      "Results [5]: [ZCTA10#2298, COMM#2281, sum(POP_2010#2290L)#2532L AS Total_Population_Zip_COMM#2533L, sum(HOUSING10#2287L)#2534L AS Total_Housing#2535L, ST_Union_Aggr(geometry#2268)#2539 AS geometry#2540]\n",
      "\n",
      "(9) Scan csv \n",
      "Output [2]: [Zip Code#2508, Estimated Median Income#2510]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "(10) Filter\n",
      "Input [2]: [Zip Code#2508, Estimated Median Income#2510]\n",
      "Condition : isnotnull(Zip Code#2508)\n",
      "\n",
      "(11) Project\n",
      "Output [2]: [Zip Code#2508, cast(regexp_replace(trim(Estimated Median Income#2510, None), [^0-9], , 1) as int) AS Estimated_Median_Income#2514]\n",
      "Input [2]: [Zip Code#2508, Estimated Median Income#2510]\n",
      "\n",
      "(12) BroadcastExchange\n",
      "Input [2]: [Zip Code#2508, Estimated_Median_Income#2514]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1696]\n",
      "\n",
      "(13) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#2298 as int)]\n",
      "Right keys [1]: [Zip Code#2508]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(14) Project\n",
      "Output [4]: [COMM#2281, Total_Population_Zip_COMM#2533L, geometry#2540, (Total_Housing#2535L * cast(Estimated_Median_Income#2514 as bigint)) AS Total income#2577L]\n",
      "Input [7]: [ZCTA10#2298, COMM#2281, Total_Population_Zip_COMM#2533L, Total_Housing#2535L, geometry#2540, Zip Code#2508, Estimated_Median_Income#2514]\n",
      "\n",
      "(15) ObjectHashAggregate\n",
      "Input [4]: [COMM#2281, Total_Population_Zip_COMM#2533L, geometry#2540, Total income#2577L]\n",
      "Keys [1]: [COMM#2281]\n",
      "Functions [3]: [partial_sum(Total_Population_Zip_COMM#2533L), partial_sum(Total income#2577L), partial_st_union_aggr(geometry#2540, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@299a121c, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#2671L, sum#2673L, buf#2675]\n",
      "Results [4]: [COMM#2281, sum#2672L, sum#2674L, buf#2676]\n",
      "\n",
      "(16) Exchange\n",
      "Input [4]: [COMM#2281, sum#2672L, sum#2674L, buf#2676]\n",
      "Arguments: hashpartitioning(COMM#2281, 1000), ENSURE_REQUIREMENTS, [plan_id=1701]\n",
      "\n",
      "(17) ObjectHashAggregate\n",
      "Input [4]: [COMM#2281, sum#2672L, sum#2674L, buf#2676]\n",
      "Keys [1]: [COMM#2281]\n",
      "Functions [3]: [sum(Total_Population_Zip_COMM#2533L), sum(Total income#2577L), st_union_aggr(geometry#2540, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@299a121c, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Total_Population_Zip_COMM#2533L)#2594L, sum(Total income#2577L)#2596L, ST_Union_Aggr(geometry#2540)#2601]\n",
      "Results [4]: [COMM#2281, sum(Total_Population_Zip_COMM#2533L)#2594L AS Total_Population#2595L, sum(Total income#2577L)#2596L AS Total Income#2597L, ST_Union_Aggr(geometry#2540)#2601 AS geometry#2602]\n",
      "\n",
      "(18) Filter\n",
      "Input [4]: [COMM#2281, Total_Population#2595L, Total Income#2597L, geometry#2602]\n",
      "Condition : isnotnull(geometry#2602)\n",
      "\n",
      "(19) Scan parquet \n",
      "Output [2]: [LAT#2428, LON#2429]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [s3://groups-bucket-dblab-905418150721/group12/main_dataset_parquet]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(20) Filter\n",
      "Input [2]: [LAT#2428, LON#2429]\n",
      "Condition : ((NOT (cast(LON#2429 as int) = 0) OR NOT (cast(LAT#2428 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(21) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#2459]\n",
      "Input [2]: [LAT#2428, LON#2429]\n",
      "\n",
      "(22) RangeJoin\n",
      "Arguments: geometry#2602: geometry, point#2459: geometry, CONTAINS\n",
      "\n",
      "(23) Project\n",
      "Output [3]: [COMM#2281, Total_Population#2595L, Total Income#2597L]\n",
      "Input [5]: [COMM#2281, Total_Population#2595L, Total Income#2597L, geometry#2602, point#2459]\n",
      "\n",
      "(24) HashAggregate\n",
      "Input [3]: [COMM#2281, Total_Population#2595L, Total Income#2597L]\n",
      "Keys [3]: [COMM#2281, Total_Population#2595L, Total Income#2597L]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#2669L]\n",
      "Results [4]: [COMM#2281, Total_Population#2595L, Total Income#2597L, count#2670L]\n",
      "\n",
      "(25) Exchange\n",
      "Input [4]: [COMM#2281, Total_Population#2595L, Total Income#2597L, count#2670L]\n",
      "Arguments: hashpartitioning(COMM#2281, Total_Population#2595L, Total Income#2597L, 1000), ENSURE_REQUIREMENTS, [plan_id=1708]\n",
      "\n",
      "(26) HashAggregate\n",
      "Input [4]: [COMM#2281, Total_Population#2595L, Total Income#2597L, count#2670L]\n",
      "Keys [3]: [COMM#2281, Total_Population#2595L, Total Income#2597L]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#2634L]\n",
      "Results [3]: [COMM#2281 AS Location#2656, format_number((cast(count(1)#2634L as double) / cast(Total_Population#2595L as double)), 4) AS crime_rate#2640, concat($, format_number(round((cast(Total Income#2597L as double) / cast(Total_Population#2595L as double)), 0), 0)) AS estimated_median_income#2646]\n",
      "\n",
      "(27) Exchange\n",
      "Input [3]: [Location#2656, crime_rate#2640, estimated_median_income#2646]\n",
      "Arguments: rangepartitioning(crime_rate#2640 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=1711]\n",
      "\n",
      "(28) Sort\n",
      "Input [3]: [Location#2656, crime_rate#2640, estimated_median_income#2646]\n",
      "Arguments: [crime_rate#2640 DESC NULLS LAST], true, 0\n",
      "\n",
      "(29) AdaptiveSparkPlan\n",
      "Output [3]: [Location#2656, crime_rate#2640, estimated_median_income#2646]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "# Explain to get the physican plan of the catalyst optimizer\n",
    "results.explain(mode=\"formatted\")\n",
    "\n",
    "# Παρατηρούμε ότι χρησιμοποιεί το Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22de3b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|Join_Strategy       |Execution_Time|\n",
      "+--------------------+--------------+\n",
      "|BROADCAST           |2.64          |\n",
      "|SHUFFLE_HASH        |4.53          |\n",
      "|SHUFFLE_REPLICATE_NL|6.16          |\n",
      "|MERGE               |7.86          |\n",
      "+--------------------+--------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, format_number\n",
    "#Print the results\n",
    "results_df = spark.read.json(\"s3://groups-bucket-dblab-905418150721/group12/Q3/hint_res.json\")\n",
    "results_df.select(col(\"Join_Strategy\"),format_number(col(\"Execution_Time\"), 2).alias(\"Execution_Time\")).orderBy(\"Execution_Time\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
