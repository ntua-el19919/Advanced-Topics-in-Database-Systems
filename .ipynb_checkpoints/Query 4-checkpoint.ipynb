{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0fd5aac-9d5c-4967-99eb-f14450870097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3641</td><td>application_1732639283265_3597</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3597/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3597_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '8g', 'spark.executor.cores': '4'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3607</td><td>application_1732639283265_3563</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3563/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3563_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3608</td><td>application_1732639283265_3564</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3564/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3564_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3612</td><td>application_1732639283265_3568</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3568/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3568_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3619</td><td>application_1732639283265_3575</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3575/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3575_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3624</td><td>application_1732639283265_3580</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3580/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3580_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3630</td><td>application_1732639283265_3586</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3586/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3586_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3634</td><td>application_1732639283265_3590</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3590/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3590_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3635</td><td>application_1732639283265_3591</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3591/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3591_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3637</td><td>application_1732639283265_3593</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3593/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3593_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3641</td><td>application_1732639283265_3597</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3597/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3597_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7fe4aba-d555-413b-bde3-eef62501f30b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 2\n",
      "Executor Memory: 8g\n",
      "Executor Cores: 4\n",
      "Top 3:\n",
      "+--------------------+-----+\n",
      "|      Victim Descent|Count|\n",
      "+--------------------+-----+\n",
      "|               White|  649|\n",
      "|               Other|   72|\n",
      "|Hispanic/Latin/Me...|   66|\n",
      "|             Unknown|   38|\n",
      "|               Black|   37|\n",
      "|         Other Asian|   21|\n",
      "|American Indian/A...|    1|\n",
      "|             Chinese|    1|\n",
      "+--------------------+-----+\n",
      "\n",
      "Bottom 3:\n",
      "+--------------------+-----+\n",
      "|      Victim Descent|Count|\n",
      "+--------------------+-----+\n",
      "|Hispanic/Latin/Me...| 2815|\n",
      "|               Black|  761|\n",
      "|               White|  330|\n",
      "|               Other|  187|\n",
      "|         Other Asian|  113|\n",
      "|             Unknown|   22|\n",
      "|American Indian/A...|   21|\n",
      "|              Korean|    5|\n",
      "|             Chinese|    3|\n",
      "|         AsianIndian|    1|\n",
      "|            Filipino|    1|\n",
      "+--------------------+-----+\n",
      "\n",
      "Execution Time:  64.47548151016235"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import (\n",
    "    col, sum, regexp_replace, substring, udf, count, to_timestamp, year\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"Q4\").getOrCreate()\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print Spark executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "# Clear Spark catalog cache\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Create Sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load GeoJSON data from S3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = (\n",
    "    sedona.read.format(\"geojson\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(geojson_path)\n",
    "    .selectExpr(\"explode(features) as features\")\n",
    "    .select(\"features.*\")\n",
    ")\n",
    "\n",
    "# Flatten the GeoJSON structure\n",
    "flattened_df = (\n",
    "    blocks_df\n",
    "    .select(\n",
    "        [col(f\"properties.{col_name}\").alias(col_name) for col_name in \n",
    "         blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]\n",
    "    )\n",
    "    .drop(\"properties\", \"type\")\n",
    ")\n",
    "\n",
    "# Filter and aggregate data for Los Angeles\n",
    "group_flattened = (\n",
    "    flattened_df\n",
    "    .select(\"COMM\", \"POP_2010\", \"ZCTA10\", \"CITY\", \"HOUSING10\", \"geometry\")\n",
    "    .filter(\n",
    "        (col(\"CITY\") == \"Los Angeles\") &\n",
    "        (col(\"ZCTA10\") > 0) &\n",
    "        (col(\"HOUSING10\") > 0) &\n",
    "        (col(\"POP_2010\") > 0) &\n",
    "        (col(\"COMM\") != \"\")\n",
    "    )\n",
    "    .groupBy(\"COMM\", \"ZCTA10\")\n",
    "    .agg(\n",
    "        sum(\"POP_2010\").alias(\"Total_POP\"),\n",
    "        sum(\"HOUSING10\").alias(\"Total_Housing\"),\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load median housing income data\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "median_housing_income = spark.read.csv(income_path, header=True, inferSchema=True)\n",
    "\n",
    "# Join census and income data and compute GDP per capita\n",
    "joined = (\n",
    "    group_flattened\n",
    "    .join(median_housing_income, group_flattened[\"ZCTA10\"] == median_housing_income[\"Zip Code\"])\n",
    "    .withColumn(\"Estimated Median Income\", regexp_replace(col(\"Estimated Median Income\"), \"[^0-9]\", \"\"))\n",
    "    .withColumn(\"ZIP_Total_Income\", (col(\"Estimated Median Income\") * col(\"Total_Housing\")))\n",
    "    .groupBy(\"COMM\")\n",
    "    .agg(\n",
    "        sum(\"Total_POP\").alias(\"Total_COMM_Pop\"),\n",
    "        sum(\"ZIP_Total_Income\").alias(\"COMM_Total_Income\"),\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "    )\n",
    "    .withColumn(\"GDP_Per_Capita\", (col(\"COMM_Total_Income\") / col(\"Total_COMM_Pop\")))\n",
    "    .select(\"COMM\", \"GDP_Per_Capita\", \"geometry\")\n",
    ")\n",
    "\n",
    "# Get top 3 and bottom 3 communities by GDP per capita\n",
    "top_3 = joined.orderBy(col(\"GDP_Per_Capita\"), ascending=False).limit(3)\n",
    "bottom_3 = joined.orderBy(col(\"GDP_Per_Capita\"), ascending=True).limit(3)\n",
    "\n",
    "# Load and filter crime data for 2015\n",
    "crime_data_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_df = spark.read.csv(crime_data_path, header=True, inferSchema=True)\n",
    "\n",
    "crime_data_df_2015 = (\n",
    "    crime_data_df\n",
    "    .withColumn(\"DATE_OCC_TIMESTAMP\", to_timestamp(col(\"DATE OCC\"), \"MM/dd/yyyy hh:mm:ss a\"))  # Convert to timestamp\n",
    "    .filter(\n",
    "        (year(col(\"DATE_OCC_TIMESTAMP\")) == 2015) &  # Extract the year\n",
    "        (col(\"Vict Descent\") != \"\")\n",
    "    )\n",
    "    .select(\"DATE OCC\", \"LAT\", \"LON\", \"Vict Descent\")\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "    .drop(\"LAT\", \"LON\")\n",
    ")\n",
    "\n",
    "# Load race code data\n",
    "race_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\"\n",
    "race = spark.read.csv(race_path, header=True, inferSchema=True)\n",
    "\n",
    "# Analyze crimes in top 3 and bottom 3 communities\n",
    "for label, community_df in [(\"Top 3\", top_3), (\"Bottom 3\", bottom_3)]:\n",
    "    crimes_df = (\n",
    "        crime_data_df_2015\n",
    "        .join(community_df, ST_Within(crime_data_df_2015.geom, community_df.geometry), \"inner\")\n",
    "        .groupBy(\"Vict Descent\")\n",
    "        .agg(count(\"*\").alias(\"Count\"))\n",
    "    )\n",
    "    final_crimes_df = (\n",
    "        crimes_df\n",
    "        .join(race, crimes_df[\"Vict Descent\"] == race[\"Vict Descent\"], \"inner\")\n",
    "        .select(race[\"Vict Descent Full\"].alias(\"Victim Descent\"), \"Count\")\n",
    "        .orderBy(col(\"Count\").desc())\n",
    "    )\n",
    "    print(f\"{label}:\")\n",
    "    final_crimes_df.show()\n",
    "\n",
    "# Log execution time and Spark configurations\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution Time: \", execution_time)\n",
    "\n",
    "# Configuration to be logged\n",
    "log_data = [{\n",
    "    \"spark_executor_instances\": conf.get(\"spark.executor.instances\"),\n",
    "    \"spark_executor_memory\": conf.get(\"spark.executor.memory\"),\n",
    "    \"spark_executor_cores\": conf.get(\"spark.executor.cores\"),\n",
    "    \"execution_time\": execution_time\n",
    "}]\n",
    "log_df = spark.createDataFrame(log_data)\n",
    "\n",
    "log_path = \"s3://groups-bucket-dblab-905418150721/group12/logfile.txt\"\n",
    "\n",
    "# Read existing log data if the file exists\n",
    "try:\n",
    "    existing_logs = spark.read.json(log_path)\n",
    "except Exception as e:\n",
    "    print(\"Log file does not exist, creating a new one.\")\n",
    "    existing_logs = spark.createDataFrame([], schema=log_df.schema)\n",
    "\n",
    "# Filter out entries with the same configuration as the new data\n",
    "filtered_logs = existing_logs.filter(\n",
    "    (col(\"spark_executor_cores\") != conf.get(\"spark.executor.cores\")) |\n",
    "    (col(\"spark_executor_memory\") != conf.get(\"spark.executor.memory\"))\n",
    ")\n",
    "\n",
    "# Combine the filtered logs with the new log entry\n",
    "updated_logs = filtered_logs.union(log_df)\n",
    "\n",
    "# Overwrite the existing log file with the updated logs\n",
    "updated_logs.write.mode(\"overwrite\").json(log_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "176917b4-b1c6-4e98-be89-4212cf996447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-----------------+\n",
      "|spark_executor_cores|spark_executor_memory|   execution_time|\n",
      "+--------------------+---------------------+-----------------+\n",
      "|                   4|                   8g|57.07837438583374|\n",
      "|                   2|                   4g|63.30001473426819|\n",
      "|                   4|                   8g|64.47548151016235|\n",
      "|                   1|                   2g|85.11916971206665|\n",
      "+--------------------+---------------------+-----------------+"
     ]
    }
   ],
   "source": [
    "log_path = f\"s3://groups-bucket-dblab-905418150721/group12/logfile.txt\"\n",
    "logs=spark.read.json(log_path)\n",
    "logs.select(\"spark_executor_cores\", \"spark_executor_memory\", \"execution_time\") \\\n",
    "    .orderBy(col(\"execution_time\").asc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca46ee-57e4-491f-b7f6-1b2c470c702d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
