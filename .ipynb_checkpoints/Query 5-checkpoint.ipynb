{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a2d0a0-827a-4b4b-a206-7c01a34760e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '8', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3608</td><td>application_1732639283265_3564</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3564/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3564_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3612</td><td>application_1732639283265_3568</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3568/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3568_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3619</td><td>application_1732639283265_3575</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3575/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3575_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3624</td><td>application_1732639283265_3580</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3580/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3580_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3630</td><td>application_1732639283265_3586</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3586/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3586_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3634</td><td>application_1732639283265_3590</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3590/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3590_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3637</td><td>application_1732639283265_3593</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3593/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3593_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3641</td><td>application_1732639283265_3597</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3597/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3597_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3653</td><td>application_1732639283265_3603</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3603/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3603_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3658</td><td>application_1732639283265_3604</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3604/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3604_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3659</td><td>application_1732639283265_3605</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3605/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3605_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3660</td><td>application_1732639283265_3606</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3606/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3606_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3661</td><td>application_1732639283265_3607</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3607/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3607_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"8\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd408a8-297c-4793-b54c-ca7faac4243a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3666</td><td>application_1732639283265_3612</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3612/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3612_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ff3c09ceea40e689e2d4f19cbce0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format,row_number, to_timestamp, round, format_number, concat, lit ,when, col, count , sum ,regexp_replace ,trim,udf,avg\n",
    "from pyspark.sql.types import FloatType,IntegerType,StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# To log our application's execution time:\n",
    "import time\n",
    "spark = SparkSession.builder.appName(\"Query 5 DF\").getOrCreate()\n",
    "spark.catalog.clearCache()\n",
    "conf = spark.sparkContext.getConf()\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "\n",
    "results = []\n",
    "# Start timing\n",
    "conf = spark.sparkContext.getConf()\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "start_time = time.time()\n",
    "#crime data\n",
    "crime_data_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_*.csv\", \n",
    "                               header=True, \n",
    "                               inferSchema=True)\n",
    "# create an ST point for each crime. Filter out points (0,0) null island\n",
    "crime_point = crime_data_df.filter(((col(\"LON\")!=0) | (col(\"LAT\")!=0))).withColumn(\"point_crime\", ST_Point(\"LON\", \"LAT\")).select(\"DR_NO\",\"point_crime\")\n",
    "print(\"Number of crimes in the start: \", crime_point.count()) \n",
    "# police stations\n",
    "police_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "police_df = spark.read.csv(police_path, header=True, inferSchema=True)\n",
    "# set an ST point for each police station\n",
    "police_df_with_point = police_df.withColumn(\"point_police\", ST_Point(\"X\", \"Y\")).select(\"point_police\",\"Division\")\n",
    "#perform crossjoin between station and crimes\n",
    "police_and_crimes = police_df_with_point.crossJoin(crime_point) \\\n",
    "                    .withColumn(\"distance_km\", ST_DistanceSphere(\"point_crime\", \"point_police\")/1000) \\\n",
    "                    .drop(\"point_police\")\n",
    "# rank pairs of crime_id and police station based on their distance\n",
    "# keep only the pairs with rank==1, meaning for each crime keep only its nearest police station\n",
    "window_spec = Window.partitionBy(\"DR_NO\").orderBy(\"distance_km\")\n",
    "crime_with_nearest_police = police_and_crimes.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "                            .filter(col(\"rank\") == 1) \\\n",
    "                            .select(\"Division\", \"distance_km\")\n",
    "final_result = crime_with_nearest_police.groupBy(\"Division\") \\\n",
    "               .agg(format_number(avg(\"distance_km\"),4).alias(\"average_distance_km\"),count(\"*\").alias(\"crime_count\"))\n",
    "print(\"Number of crimes in the end: \", final_result.agg(sum(\"crime_count\")).collect()[0][0])\n",
    "final_result.orderBy(col(\"crime_count\").desc()).show()\n",
    "end_time = time.time()\n",
    "exec_time = end_time - start_time\n",
    "results.append({\n",
    "    \"Executor instances\": conf.get(\"spark.executor.instances\"),\n",
    "    \"cores\": conf.get(\"spark.executor.cores\"),\n",
    "    \"memory\": conf.get(\"spark.executor.memory\"),\n",
    "    \"time\": exec_time\n",
    "})\n",
    "# Assuming `results` is a list of dictionaries (if you're not using a DataFrame)\n",
    "results_df = spark.createDataFrame(results)\n",
    "\n",
    "# Save the DataFrame to S3 as JSON\n",
    "results_df.write.mode(\"append\").json(\"s3://groups-bucket-dblab-905418150721/group12/q5_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe3206-1f38-4b3a-850f-0dc36140aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = f\"s3://groups-bucket-dblab-905418150721/group12/q5_results.json\"\n",
    "logs=spark.read.json(log_path)\n",
    "logs.select(\"Executor instances\", \"cores\", \"memory\", \"time\") \\\n",
    "    .orderBy(col(\"time\").asc()) \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
